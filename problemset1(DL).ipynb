{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름/학번\n",
    "\n",
    "이름: 김수현\n",
    "\n",
    "학번: 20183911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example dataset\n",
    "\n",
    "강의를 위해서 임의의 dataset을 준비하겠습니다.\n",
    "예제로 봐주시고, 큰 물리적 의미는 부여하지 않겠습니다.\n",
    "\n",
    "- Data는 장미과와 국화과의 A 효소, B 효소, C 효소, D 효소를 측정한 값이라고 가정합니다.\n",
    "- Label은 각 sample이 장미인지 (0) 국화인지 (1)에 대한 정보라고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "num_feature = 4\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_batch = torch.randn(batch_size, num_feature)\n",
    "Y_batch = (torch.sum(X_batch, dim=1)>0).type(torch.float).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
       "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
       "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
       "        [ 0.1198,  1.2377,  1.1168, -0.2473],\n",
       "        [-1.3527, -1.6959,  0.5667,  0.7935],\n",
       "        [ 0.5988, -1.5551, -0.3414,  1.8530],\n",
       "        [-0.2159, -0.7425,  0.5627,  0.2596],\n",
       "        [-0.1740, -0.6787,  0.9383,  0.4889],\n",
       "        [ 1.2032,  0.0845, -1.2001, -0.0048],\n",
       "        [-0.5181, -0.3067, -1.5810,  1.7066]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation 정리\n",
    "\n",
    "강의자료와 비교하면 \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{X_batch} = \n",
    "\\begin{bmatrix}\n",
    "(x^{(1)})^\\top\\\\\n",
    "(x^{(2)})^\\top\\\\\n",
    "\\vdots \\\\\n",
    "(x^{(m)})^\\top\n",
    "\\end{bmatrix}, \\quad\n",
    "\\text{Y_batch} = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Problem: Single Neuron\n",
    "\n",
    "- 한개의 neuron이 있다고 가정하고 $\\mathbb{R}^4$ 를 입력받아서 $\\mathbb{R}$로 출력한다고 가정합니다.\n",
    "- Activation 함수는 ReLU 함수, 즉, \n",
    "\\begin{align*}\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "를 사용한다고 가정합니다.\n",
    "\n",
    "Neuron을 통해서 batch sample 전체를\n",
    "\\begin{align*}\n",
    "Z = \\begin{bmatrix}\n",
    "(w^T x^{(1)} + b)^T \\\\\n",
    "(w^T x^{(2)} + b)^T \\\\\n",
    "\\vdots \\\\\n",
    "(w^T x^{(m)} + b)^T\n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "연산을 수행해서 $Z$를 구하세요.\n",
    "\n",
    "- $w$는 random Gaussian으로 생성하세요. 위에서 예기한 입력과 출력이 맞도록 weight를 생성하세요.\n",
    "- Bias $b$는 1로 설정\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2910],\n",
      "        [ 3.0975],\n",
      "        [ 1.6780],\n",
      "        [ 1.0120],\n",
      "        [ 1.4991],\n",
      "        [-0.5044],\n",
      "        [ 1.4891],\n",
      "        [ 1.3363],\n",
      "        [ 0.2980],\n",
      "        [-1.8288]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "W = torch.randn(num_feature, 1)\n",
    "b = 1\n",
    "Z = torch.matmul(X_batch, W)+b\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 연산을 하는지 확인하도록 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2910],\n",
      "        [ 3.0975],\n",
      "        [ 1.6780],\n",
      "        [ 1.0120],\n",
      "        [ 1.4991],\n",
      "        [-0.5044],\n",
      "        [ 1.4891],\n",
      "        [ 1.3363],\n",
      "        [ 0.2980],\n",
      "        [-1.8288]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_15332\\1446079948.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  z_loop[i,:] = torch.matmul(W.T, X_batch[i,:].T)+b\n"
     ]
    }
   ],
   "source": [
    "z_loop = torch.empty(batch_size, 1)\n",
    "for i in torch.arange(batch_size):\n",
    "    z_loop[i,:] = torch.matmul(W.T, X_batch[i,:].T)+b\n",
    "print(z_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "ReLU 함수를 작성하고 위에서 찾은 $Z$의 각 원소에 ReLU 함수를 적용하여 `a`라는 변수에 저장하세요.\n",
    "\n",
    "- torch.clamp() 함수를 공부하고 적용하세요\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2910],\n",
      "        [3.0975],\n",
      "        [1.6780],\n",
      "        [1.0120],\n",
      "        [1.4991],\n",
      "        [0.0000],\n",
      "        [1.4891],\n",
      "        [1.3363],\n",
      "        [0.2980],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성:\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.clamp(x, min = 0)\n",
    "A = ReLU(Z)\n",
    "print(A)\n",
    "\n",
    "\n",
    "# torch.clamp(tensor, min= , max=) 형태로 값을 지정할 수 있는데 ReLU 함수의 경우 최대값을 지정할 필요가 없어서 min 값만 지정해줬습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Network\n",
    "\n",
    "- 한개의 Layer에 $k=5$개의 Neuron 이 있는 network를 구성하고 출력을 구하세요\n",
    "- Activation function은 모든 neuron에 ReLU를 적용합니다\n",
    "- 모든 weight는 Gaussian 분포로 랜덤 생성하세요 `torch.randn()`\n",
    "- $i$ 번째 neuron의 weight들을 $w_i$라고 할때,\n",
    "\\begin{align*}\n",
    "\\text{W} = \\begin{bmatrix}\n",
    "w_1, w_2, w_3, w_4, w_5\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "라고 하고, weight matrix `W`를 만드세요.\n",
    "  - `W = torch.randn(???, ???)` 으로 생성\n",
    "- Bias 역시 `b`라는 `tensor`에 저장하고, 각 neuron 별로 `1`로 설정합니다\n",
    "  - `b = torch.ones(???,???)`\n",
    "- 아래 problem 2-2에서 수업에서 배운 $Z$ 행렬과 $A$ 행렬을 구하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-1\n",
    "`Z` 행렬과 `A` 행렬의 차원은 어떻게 나와야하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답 작성)\n",
    "\n",
    "Z 는 (10,5), \n",
    "A 는 (10,5)\n",
    "\n",
    "-> Z행렬에 ReLU를 적용한 것이 A행렬이기 때문에 Z행렬과 A행렬은 차원이 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-2 \n",
    "위에서 요구한 코딩을 완성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5009, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 5.6229, 3.3937, 0.3508, 0.0000],\n",
      "        [1.9497, 0.0000, 0.4126, 1.3829, 2.4589],\n",
      "        [1.5663, 3.2170, 0.5114, 0.0000, 0.0751],\n",
      "        [3.6741, 0.0000, 0.0000, 0.0000, 2.0759],\n",
      "        [2.1981, 0.0000, 0.7003, 3.8946, 5.3346],\n",
      "        [2.2619, 0.0000, 0.0000, 0.4367, 1.6558],\n",
      "        [2.8142, 0.0000, 0.0000, 0.3528, 2.0421],\n",
      "        [0.0000, 1.7287, 3.6229, 3.6950, 1.9198],\n",
      "        [0.7818, 0.0000, 1.2625, 3.4941, 3.4477]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    " \n",
    "W = torch.randn(4,5)\n",
    "b = torch.ones(1,5)\n",
    "Z = torch.matmul(X_batch, W)+b\n",
    "A = ReLU(Z) \n",
    "\n",
    "print(A)\n",
    "\n",
    "# W는 input으로 들어오는 값이 행의 개수, output으로 나가는 값이 열의 개수입니다.\n",
    "# b는 각 뉴런 별 1로 설정했기 때문에 5개의 뉴런에 1을 더해 줄 수 있도록 길이가 5인 (python)1차원 벡터를 만들어줍니다.\n",
    "# Z는 X와 W의 선형결합에 b를 더해준 값으로 나타납니다.\n",
    "# A는 ReLU(Z)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: \n",
    "\n",
    "- $x^{(3)}$ 를 입력으로하는 2번째 Neuron의 결과값을 출력하세요\n",
    "- 위에서 구한 `A[i, j]`  인덱싱을 통해서 출력하세요\n",
    "- `Python`의 인덱싱은 `0`부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer =  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "print('answer = ', A[2, 1])\n",
    "# x^(i),i=1,2,...,10 으로 각 x는 위 문제2번 결과의 각 행이고, 5개의 뉴런을 통해 나온 선형연산의 결과는 열로 나타납니다.\n",
    "# x^(3)을 입력으로 하는 연산 결과는  A[2,:]이고, 3번째 Neuron의 연산 결과는 A[:,1]이므로 x^(3)을 입력으로 하는 2번째 Neuron의 결과값은 A[2,1]입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Multi-Layer network\n",
    "\n",
    "- 3개의 layer가 있는 network를 구성합니다.\n",
    "- 2번째 layer의 입력 크기는 $k^{[1]}=16$, 출력 크기는 $k^{[2]}=6$\n",
    "- 마지막 layer의 출력은 $k^{[3]}=1$개의 neuron으로 구성\n",
    "- 각 layer의 연산값을 구하세요. \n",
    "  - 각 layer의 선형 변환 결과값은 `Z1`, `Z2`, `Z3`로 저장하세요\n",
    "  - 각 layer의 결과값은 `A1`, `A2`, `A3`로 저장하세요\n",
    "- 모든 weight는 Gaussian 랜덤 변수로 생성, bias는 1로 구성된 벡터로 생성함\n",
    "- 각 layer의 weight는 `W1`, `W2`, `W3`로하고, bias는 `b1`, `b2`, `b3`로 생성함\n",
    "- Activation 함수는 `ReLU`를 적용하세요\n",
    "- Loop 없이 행렬연산으로 구생하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "W1= torch.randn(4,16)\n",
    "W2= torch.randn(16,6)\n",
    "W3= torch.randn(6,1) \n",
    "b1= torch.ones(1,16) \n",
    "b2= torch.ones(1,6)\n",
    "b3= 1\n",
    "\n",
    "Z1 = torch.matmul(X_batch, W1)+b1 \n",
    "A1 = ReLU(Z1)\n",
    "Z2 = torch.matmul(A1,W2)+b2\n",
    "A2 = ReLU(Z2) \n",
    "Z3 = torch.matmul(A2,W3)+b3\n",
    "A3 = ReLU(Z3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16])\n",
      "tensor([[ 2.5549e+00,  2.2229e+00,  1.9173e+00,  1.0671e+00,  1.8000e+00,\n",
      "         -8.1495e-01,  4.8913e-01,  2.3724e+00,  6.7394e-01,  7.5445e-01,\n",
      "         -9.3364e-01,  2.1596e-01, -1.1603e+00, -9.3651e-02,  9.8412e-01,\n",
      "          1.8653e+00],\n",
      "        [-1.3552e+00, -1.1635e+00, -2.2510e-02,  1.7841e+00,  1.9706e+00,\n",
      "          5.6673e+00, -2.8499e+00,  7.0236e-01,  2.9956e+00,  7.3250e-01,\n",
      "          2.0680e+00,  2.8065e-01,  7.9413e-01, -1.2854e+00,  1.7711e-01,\n",
      "          3.1582e-01],\n",
      "        [-7.5565e-01,  2.9468e-01,  2.7113e+00,  7.5281e-01,  2.0534e+00,\n",
      "         -4.1597e-01,  2.6103e+00,  2.1627e+00,  7.3798e-01, -1.0151e+00,\n",
      "          2.4346e-02, -8.4654e-01, -3.6184e-01,  7.5211e-01,  2.7299e-02,\n",
      "          2.3964e+00],\n",
      "        [ 2.4884e+00,  1.6547e+00,  2.0604e+00, -1.9408e-01,  1.6191e+00,\n",
      "          2.6641e+00,  6.9943e-01, -2.6340e+00,  1.2683e+00,  1.1058e+00,\n",
      "          1.8484e+00,  2.7851e+00,  3.0059e+00,  2.7595e+00,  1.2777e-01,\n",
      "         -2.0744e-01],\n",
      "        [ 3.4774e+00,  3.2739e+00,  3.8173e+00,  2.0639e-02,  2.0787e+00,\n",
      "         -3.4317e+00,  3.3464e+00,  1.6724e+00, -4.4493e-01, -1.4077e-01,\n",
      "         -1.6751e+00,  3.4593e-01, -9.4225e-01,  1.7482e+00,  5.5565e-01,\n",
      "          2.4924e+00],\n",
      "        [-1.7354e+00, -1.9133e-03,  1.2578e+00,  1.3994e+00,  2.0871e-01,\n",
      "         -2.5224e+00,  4.8732e+00,  4.1978e+00, -1.8926e-01, -3.9358e-01,\n",
      "          4.3032e-01, -1.0822e+00, -8.0474e-02,  1.2478e+00,  1.3995e+00,\n",
      "          2.8531e+00],\n",
      "        [ 1.2678e+00,  1.4700e+00,  2.7790e+00,  3.5245e-01,  1.9315e+00,\n",
      "         -4.2606e-01,  2.2204e+00,  8.6214e-01,  5.6906e-01, -1.7800e-01,\n",
      "          1.0630e-02,  4.0829e-01,  2.9560e-01,  1.4609e+00,  2.3677e-01,\n",
      "          1.7460e+00],\n",
      "        [ 1.5151e+00,  1.6852e+00,  3.3971e+00, -6.5674e-02,  2.1279e+00,\n",
      "         -6.6962e-01,  2.8407e+00,  1.4525e-01,  3.9631e-01, -4.5593e-01,\n",
      "          2.0955e-02,  6.3861e-01,  6.6513e-01,  2.1314e+00, -5.5331e-02,\n",
      "          1.7231e+00],\n",
      "        [-2.7969e+00, -1.5739e+00, -1.6926e+00,  2.6085e+00, -3.3578e-01,\n",
      "          2.5418e+00,  4.0496e-01,  3.6282e+00,  1.6862e+00,  1.2961e+00,\n",
      "          2.3926e+00, -2.9468e-01,  9.0919e-01, -6.7186e-01,  1.8932e+00,\n",
      "          1.1489e+00],\n",
      "        [ 1.5862e+00,  1.6935e+00, -2.0263e+00,  2.3517e+00, -2.0411e+00,\n",
      "         -2.1543e+00,  2.6616e+00,  4.3787e+00, -3.1530e-01,  3.2622e+00,\n",
      "          1.0749e+00,  1.3237e+00,  8.5403e-01,  9.3251e-01,  3.8105e+00,\n",
      "          1.2661e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(Z1.shape)\n",
    "print(Z1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6])\n",
      "tensor([[16.9212,  1.0260, -4.6165, -6.3777,  5.9811, -0.3279],\n",
      "        [ 7.6355,  7.2746,  3.5961, -3.8283,  2.1788,  6.9361],\n",
      "        [ 9.2321, -1.2725, -7.4480, -7.8591, 11.3802,  0.7822],\n",
      "        [10.8815,  1.3737,  6.1209,  0.0476, -0.7056, -6.3161],\n",
      "        [19.8907, -2.5386, -6.3575, -5.6670, 11.8698, -5.2738],\n",
      "        [12.6349, -6.8004, -4.1671, -8.0727,  8.2638,  0.3457],\n",
      "        [11.3866, -0.5714, -4.4427, -5.3309,  9.6460, -2.6888],\n",
      "        [11.4696, -1.1531, -3.9394, -4.8390, 10.6136, -4.9896],\n",
      "        [12.8070,  3.3260,  4.4189, -6.3869, -0.6832,  6.1099],\n",
      "        [22.6441, -6.5984,  1.9180, -5.6541, -3.5610, -3.2025]])\n"
     ]
    }
   ],
   "source": [
    "print(Z2.shape)\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "tensor([[ -3.0082],\n",
      "        [ -7.8443],\n",
      "        [ -1.4784],\n",
      "        [-13.5116],\n",
      "        [ -3.6889],\n",
      "        [ -1.9003],\n",
      "        [ -2.0068],\n",
      "        [ -2.1303],\n",
      "        [ -8.6048],\n",
      "        [ -6.5886]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(Z3.shape)\n",
    "print(Z3)\n",
    "print(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "- 위에서 구한 `A3[i,j]`의 인덱싱을 통해서 $h_\\theta(x^{(3)})$ 을 출력하세요\n",
    "- 역시 `python`인덱싱은 `0` 부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_theta(x3) =  tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "print('h_theta(x3) = ', A3[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "위에서 공부한 것을 함수로 만들어 보도록 하겠습니다.\n",
    "아래 한 layer의 선형 변환을 연산하는 class를 만들어 보도록 하죠.\n",
    "\n",
    "- Class는 `my_linear_layer()`\n",
    "  - `__init__(self, n_input, n_output)` 함수:\n",
    "    - `self.W` 변수 초기화: Weight 행렬 `self.W`를 램덤 Gaussian 생성 (차원에 맞는...)\n",
    "    - `self.b` 변수 초기화: bias 벡터 `self.b`를 모두 `1`인 벡터 생성 (차원에 맞는...)\n",
    "  - `forward(A)` 함수:\n",
    "    - 입력: `A`는 sample batch $X$ 또는 전 layer에서 들어오는 입력 batch $A^{[\\ell-1]}$을 입력하는 자리\n",
    "    - return 값\n",
    "      - `Z` 변수는 $A^{[\\ell-1]}$의 선형 변환 값, 즉 $Z^{[\\ell]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "class my_linear_layer():\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = torch.randn(n_input, n_output)\n",
    "        self.b = torch.ones(1, n_output)\n",
    "    \n",
    "    def forward(self,A):\n",
    "        Z = torch.matmul(A,self.W)+self.b\n",
    "        return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답을 확인하기 위해서 `n_input=num_feature`과 `n_output = 5` 인 `my_linear_layer` instance 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0638,  2.2198, -1.7476, -0.3184,  0.2471],\n",
       "        [ 0.2967, -2.2693, -1.0325, -3.7866,  0.9864],\n",
       "        [ 2.8471,  1.4401, -0.5475,  1.4418,  1.7400],\n",
       "        [ 0.7613,  0.5061,  4.5036,  2.8031,  1.9201],\n",
       "        [ 5.3088,  4.0278, -0.0173,  3.2829,  1.0363],\n",
       "        [ 0.6880,  2.4766, -0.1730,  3.1860,  1.3804],\n",
       "        [ 2.9911,  1.8939,  0.7868,  2.1593,  1.5343],\n",
       "        [ 3.1934,  2.1373,  1.7049,  3.2263,  1.9170],\n",
       "        [-2.2147, -0.7392, -0.8183, -1.3959,  0.5713],\n",
       "        [-1.6620,  2.8714,  0.5136,  1.9950, -0.6663]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "mll = my_linear_layer(num_feature, 5)\n",
    "mll.forward(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.5988],\n",
      "        [-1.5551, -0.3414,  1.8530,  0.4681, -0.1577],\n",
      "        [ 1.4437,  0.2660,  1.3894,  1.5863,  0.9463],\n",
      "        [-0.8437,  0.9318,  1.2590,  2.0050,  0.0537]])\n",
      "tensor([[1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(mll.W)\n",
    "print(mll.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Linear Layer with `torch.nn`\n",
    "위에서 수행한 작업을 `pytorch`에서는 `torch.nn.Linear`라는 명령어로 쉽게 구현할 수 있습니다.\n",
    "아래 예제를 보도록 하죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2346,  0.0595,  0.1510, -0.2836, -0.0313],\n",
       "        [-0.4402,  0.4180, -1.5513, -1.6114,  0.9582],\n",
       "        [-0.3911, -0.1696,  0.0634, -0.0419,  0.2315],\n",
       "        [ 0.7794, -0.4919, -0.4290, -0.0730,  0.7808],\n",
       "        [ 0.0778, -0.3110,  0.9907,  0.6479, -0.2004],\n",
       "        [-0.3771, -0.8672,  0.2735,  0.4489, -0.4199],\n",
       "        [ 0.0255, -0.2766,  0.1765,  0.1120,  0.2455],\n",
       "        [ 0.1800, -0.3671,  0.3306,  0.3356,  0.3165],\n",
       "        [-0.4906, -0.4770, -1.1830, -0.9128,  0.0698],\n",
       "        [ 0.1734, -1.2531, -0.0787,  0.2049, -0.9297]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "#W = torch.randn(num_feature, 5)\n",
    "L1 = nn.Linear(num_feature, 5)\n",
    "Zll = L1(X_batch)\n",
    "Zll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5009, -0.6663, -0.2224, -0.6280, -0.0453],\n",
       "        [-1.6313,  5.6229,  3.3937,  0.3508, -2.1027],\n",
       "        [ 1.9497, -0.3517,  0.4126,  1.3829,  2.4589],\n",
       "        [ 1.5663,  3.2170,  0.5114, -0.1567,  0.0751],\n",
       "        [ 3.6741, -3.0020, -2.0296, -0.5144,  2.0759],\n",
       "        [ 2.1981, -3.0451,  0.7003,  3.8946,  5.3346],\n",
       "        [ 2.2619, -0.1470, -0.2009,  0.4367,  1.6558],\n",
       "        [ 2.8142, -0.2460, -0.6185,  0.3528,  2.0421],\n",
       "        [-1.1247,  1.7287,  3.6229,  3.6950,  1.9198],\n",
       "        [ 0.7818, -3.0393,  1.2625,  3.4941,  3.4477]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1.weight = nn.Parameter(W.T)\n",
    "L1.bias.data.fill_(1.0)\n",
    "Zll2 = L1(X_batch)\n",
    "Zll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5009, -0.6663, -0.2224, -0.6280, -0.0453],\n",
       "        [-1.6313,  5.6229,  3.3937,  0.3508, -2.1027],\n",
       "        [ 1.9497, -0.3517,  0.4126,  1.3829,  2.4589],\n",
       "        [ 1.5663,  3.2170,  0.5114, -0.1567,  0.0751],\n",
       "        [ 3.6741, -3.0020, -2.0296, -0.5144,  2.0759],\n",
       "        [ 2.1981, -3.0451,  0.7003,  3.8946,  5.3346],\n",
       "        [ 2.2619, -0.1470, -0.2009,  0.4367,  1.6558],\n",
       "        [ 2.8142, -0.2460, -0.6185,  0.3528,  2.0421],\n",
       "        [-1.1247,  1.7287,  3.6229,  3.6950,  1.9198],\n",
       "        [ 0.7818, -3.0393,  1.2625,  3.4941,  3.4477]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
